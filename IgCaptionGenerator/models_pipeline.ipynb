{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "import os\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP Caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: C:\\Users\\julka\\OneDrive\\Pulpit\\our_img37.jpg\n",
      "BLIP Caption: there is a man standing on a rock with a surfboard\n"
     ]
    }
   ],
   "source": [
    "def generate_blip_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = blip_model.generate(pixel_values)\n",
    "        blip_caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return blip_caption\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\julka\\\\OneDrive\\\\Pulpit\\\\our_img37.jpg\"\n",
    "blip_caption = generate_blip_caption(image_path)\n",
    "print(\"Image:\", image_path)\n",
    "print(\"BLIP Caption:\", blip_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "def load_captionize_model(model_path):\n",
    "    return load_model(model_path)\n",
    "\n",
    "our_model_path = \"C:\\\\Users\\\\julka\\\\OneDrive\\\\Pulpit\\\\caption_model12.keras\"\n",
    "our_tokenizer_path = \"C:\\\\Users\\\\julka\\\\OneDrive\\\\Pulpit\\\\tokenizer.pickle\"\n",
    "\n",
    "our_model = load_captionize_model(our_model_path)\n",
    "our_tokenizer = load_tokenizer(our_tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Image: C:\\Users\\julka\\OneDrive\\Pulpit\\our_img37.jpg\n",
      "Our Caption: <start> we had a blast show <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_incep = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_features(image_path):\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    features = model_incep.predict(img_array)\n",
    "\n",
    "    return features\n",
    "\n",
    "def top_k_sampling(predictions, k=5, unk_token='<unk>'):\n",
    "    unk_index = our_tokenizer.word_index.get(unk_token, None)\n",
    "    if unk_index is not None:\n",
    "        predictions[unk_index] = 0 \n",
    "\n",
    "    k = min(k, len(predictions))\n",
    "\n",
    "    top_k_indices = np.argsort(predictions)[-k:]\n",
    "    top_k_probs = predictions[top_k_indices]\n",
    "    top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
    "    chosen_index = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "    return chosen_index\n",
    "\n",
    "def generate_caption(our_model, our_tokenizer, photo, max_length, k=5):\n",
    "    start_token = '<start>'\n",
    "    end_token = '<end>'\n",
    "    in_text = start_token\n",
    "    for _ in range(max_length):\n",
    "        sequence = our_tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        predictions = our_model.predict([photo, sequence], verbose=0)[0]\n",
    "        \n",
    "        prediction = top_k_sampling(predictions, k)\n",
    "        word = our_tokenizer.index_word.get(prediction, None)\n",
    "\n",
    "        if word == end_token and in_text == start_token:\n",
    "            continue\n",
    "        if word == in_text[-1]:\n",
    "            break\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == end_token:\n",
    "            break\n",
    "        \n",
    "    return in_text\n",
    "\n",
    "def generate_captionize_caption(image_path):\n",
    "    captionize_image = extract_features(image_path)\n",
    "    captionize_caption = generate_caption(our_model, our_tokenizer, captionize_image, 60)\n",
    "\n",
    "    return captionize_caption\n",
    "\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\julka\\\\OneDrive\\\\Pulpit\\\\our_img37.jpg\"\n",
    "\n",
    "captionize_caption = generate_captionize_caption(image_path)\n",
    "\n",
    "print(f\"Image: {image_path}\")   \n",
    "print(f\"Our Caption: {captionize_caption}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    #device_map=\"auto\" - uncomment it on google colab\n",
    ")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Caption: \"Surfing together, man! #Showtime!\"\n"
     ]
    }
   ],
   "source": [
    "def generate_combined_caption(blip_caption, our_caption):\n",
    "    our_caption_cleaned = our_caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Combine the following two captions into a single, catchy, Instagram-worthy caption, without emojis and without hashtags. Make sure to use both captions:\n",
    "\n",
    "    Caption 1: {blip_caption}\n",
    "    Caption 2: {our_caption_cleaned}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = qwen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = qwen_tokenizer([text], return_tensors=\"pt\").to(qwen_model.device)\n",
    "\n",
    "    generated_ids = qwen_model.generate(**model_inputs)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    response = qwen_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "final_caption = generate_combined_caption(blip_caption, captionize_caption)\n",
    "\n",
    "print(\"Final Caption:\", final_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip caption: three women are posing for a picture in front of a building\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Our caption: <start> i miss u <end>\n",
      "Final (Qwen) caption: \"Take this moment together! üò§ #LoveLost #StayInTouch\"\n"
     ]
    }
   ],
   "source": [
    "def process_image_and_generate_caption(image_path):\n",
    "    # blip\n",
    "    blip_caption = generate_blip_caption(image_path)\n",
    "    print(\"Blip caption:\", blip_caption)\n",
    "\n",
    "    # captionize\n",
    "    captionize_caption = generate_captionize_caption(image_path)\n",
    "    print(\"Our caption:\", captionize_caption)\n",
    "\n",
    "    # qwen\n",
    "    final_caption = generate_combined_caption(blip_caption, captionize_caption)\n",
    "    print(\"Final (Qwen) caption:\", final_caption)\n",
    "\n",
    "    return final_caption\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\julka\\\\OneDrive\\\\Pulpit\\\\our_img10.jpg\"\n",
    "\n",
    "final_caption = process_image_and_generate_caption(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
